{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test Value Function Approximation method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-08 14:22:50.237894: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-08-08 14:22:50.329716: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-08-08 14:22:52.041834: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import gymnasium as gym \n",
    "import matplotlib.pyplot as plt \n",
    "import seaborn as sns \n",
    "import pandas as pd \n",
    "import numpy as np \n",
    "import torch\n",
    "import tensorflow as tf \n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import os\n",
    "import timeit\n",
    "import datetime\n",
    "from pathlib import Path\n",
    "\n",
    "from environment import VFAFarmEnv\n",
    "from algorithms import DQNAgent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define path to saving directory\n",
    "TrainedDQLearning_model = Path('/home/elisa/Desktop/Uni/SecondY/RL/SecondPart/SummerClaude/TrainedDQLearning/Model')\n",
    "TrainedDQLearning_data = Path('/home/elisa/Desktop/Uni/SecondY/RL/SecondPart/SummerClaude/TrainedDQLearning/Data')\n",
    "TrainedDQLearning_plots = Path('/home/elisa/Desktop/Uni/SecondY/RL/SecondPart/SummerClaude/TrainedDQLearning/Plots') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/elisa/Desktop/Uni/SecondY/RL/SecondPart/Final/final/lib/python3.10/site-packages/keras/src/layers/core/dense.py:86: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n",
      "2024-08-08 14:23:55.388470: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:998] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-08-08 14:23:55.390120: W tensorflow/core/common_runtime/gpu/gpu_device.cc:2251] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n"
     ]
    }
   ],
   "source": [
    "#initialize agent and environment\n",
    "env = gym.make('VFAFarm-v0')\n",
    "agent = DQNAgent(env)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training log"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Log for training**\n",
    "* first training 500 episodes, -v0 environment, no replay buffer, parameters:    `learning_rate=0.001, gamma=0.99, epsilon=1.0, epsilon_decay=0.995, epsilon_min=0.01, batch_size=32, memory_size=10000`\n",
    "\n",
    "* 20240809_205157: 1000 episodes, -v00 environment, no replay buffer, parameters: `learning_rate=0.001, gamma=0.99, epsilon=1.0, epsilon_decay=0.995, epsilon_min=0.01, batch_size=32, memory_size=10000`\n",
    "\n",
    "from this point are available also information on the environment evolution during training\n",
    "\n",
    "* 20240810_123124: 100 episodes, -v00 environment, no replay buffer, parameters: `self, initial_sheep=0, initial_budget=2000, initial_year=0, prob_storm=0.3, prob_sheep=0.1, sheep_cost=1000, wheat_cost=20`\n",
    "\n",
    "* 20240810_125246: 100 episodes, -v00 environment, no replay buffer, parameters: `self, initial_sheep=0, initial_budget=2000, initial_year=0, prob_storm=0.3, prob_sheep=0.1, sheep_cost=1000, wheat_cost=20` *with respect to previous run the model.save method was corrected*\n",
    "\n",
    "* 20240811_091314 : 1000 episodes, -v00 environment, replay buffer, parameters: `self, initial_sheep=0, initial_budget=2000, initial_year=0, prob_storm=0.3, prob_sheep=0.1, sheep_cost=1000, wheat_cost=20`\n",
    "\n",
    "from here on the environment is changed and some information are available in the history\n",
    "\n",
    "* 20240819_183248: 1000 episode, -v01 environment, no replay buffer, parameters: `self, env, model=None, learning_rate=0.001, gamma=0.99, epsilon=1.0, epsilon_decay=0.995, epsilon_min=0.01, batch_size=32, memory_size=10000`\n",
    "\n",
    "* DQN_final_20240821_180519: 1000 episode, -v01 environment, replay buffer, parameters: `self, env, model=None, learning_rate=0.001, gamma=0.99, epsilon=1.0, epsilon_decay=0.995, epsilon_min=0.01, batch_size=32, memory_size=10000`\n",
    "\n",
    "* REINFORCE_final_20240822_161459: 1000 episode, -v01 environment, REINFORCE, parameters: `self, env, model=None, learning_rate=0.001, gamma=0.99, epsilon=1.0, epsilon_decay=0.995, epsilon_min=0.01, batch_size=32, memory_size=10000`\n",
    "\n",
    "\n",
    "* REINFORCE_final_20241007_104103: 1000 episode, -v01 environment, REINFORCE, parameters: `self, env, model=None, learning_rate=0.001, gamma=0.99, epsilon=1.0, epsilon_decay=0.995, epsilon_min=0.01, batch_size=32, memory_size=10000`- REINFORCE on Galileo\n",
    "\n",
    "\n",
    "* DQN_final_20241010_212023: 1000 episode, -v01 environment, replay buffer, parameters: `self, env, model=None, learning_rate=0.001, gamma=0.99, epsilon=1.0, epsilon_decay=0.995, epsilon_min=0.01, batch_size=32, memory_size=10000`- DQN on Galielo - missing error\n",
    "\n",
    "Fino a questo momento il reward Ã¨ stato calcolato come differenza tra il budget a inizio episodio e il budget alla fine dell'anno, da qui in avanti sono stati ripetuti i tre tipi di training principali ma con un nuovo meccanismo di reward\n",
    "\n",
    "* TO CHANGE:  DQN_final_20241010_212023: 1000 episode, -v01 environment, replay buffer, parameters: `self, env, model=None, learning_rate=0.001, gamma=0.99, epsilon=1.0, epsilon_decay=0.995, epsilon_min=0.01, batch_size=32, memory_size=10000`- DQN on Galielo - missing error\n",
    "\n",
    "* DQN_final_20241027_081302: 1000 episode, -v01 environment, replay buffer, parameters: `self, env, model=None, learning_rate=0.001, gamma=0.99, epsilon=1.0, epsilon_decay=0.995, epsilon_min=0.01, batch_size=32, memory_size=10000`- DQN on Galielo - missing error\n",
    "\n",
    "A questo punto ho cambiato il valore massimo del budget a 20 0000\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "final",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
